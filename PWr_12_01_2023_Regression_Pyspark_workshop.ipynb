{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamskyy/labs/blob/main/PWr_12_01_2023_Regression_Pyspark_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twmNEYCEJeN8"
      },
      "source": [
        "# Install Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkkwNxc6tqoI"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISCMmr09trA_"
      },
      "source": [
        "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xxXKkE9tsIM"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82LTKkMVttm7"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ_-u_vJJrJR"
      },
      "source": [
        "# Create session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YdIQ8nftwWt"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql import Window\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('DataFrame') \\\n",
        "    .master('local[*]') \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtA5OfSyJt6D"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZBPGuDwtzCB"
      },
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDgylcfmrlR5"
      },
      "source": [
        "Task: \n",
        "Using adults data, predict if the yearly income will be above 50K $.\n",
        "\n",
        "Label: earnings (can have two values <=50K, >50K).\n",
        "Features: other columns. \n",
        "\n",
        "It is binary classification task and we can use for linear regressison to solve it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz7c3SP6uYUO"
      },
      "source": [
        "col_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\"marital-status\", \"occupation\", \n",
        "             \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \n",
        "             \"native-country\", \"earnings\"]\n",
        "\n",
        "df = spark.read.csv(\"adult.data\", header=False, inferSchema=True, ignoreLeadingWhiteSpace=True)\n",
        "df = df.select(*[f.col(old).alias(new) for old, new in zip(df.columns, col_names)]).drop(\"fnlwgt\").dropna(\"any\")\n",
        "df.show(3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropDuplicates(['earnings']).show(3)"
      ],
      "metadata": {
        "id": "XTUiI8BfXAwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWsbe5pqHbv"
      },
      "source": [
        "# Divide data to train and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUDY1298VrZ6"
      },
      "source": [
        "(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(trainingData.count())\n",
        "print(testData.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cIxUSmJSkGF"
      },
      "source": [
        "# ML pipeline creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "687JFwfGFZCk"
      },
      "source": [
        "For data processing and model training we will use Spark Piepline API. \n",
        "Each transformation on dataset needs to be added to pipeline stages list. \n",
        "\n",
        "Steps:\n",
        "1. Replace categorical columns with discrete encodings lLabels encoding). We will use **StringIndexer** https://spark.apache.org/docs/1.5.1/ml-features.html \n",
        "\n",
        "```\n",
        "  #  id | category | category_index\n",
        "  # ----|----------|---------------\n",
        "  #  0  | a        | 0.0\n",
        "  #  1  | b        | 2.0\n",
        "  #  2  | c        | 1.0\n",
        "  #  3  | a        | 0.0\n",
        "  #  4  | a        | 0.0\n",
        "  #  5  | c        | 1.0\n",
        "```\n",
        "\n",
        "2. One hot encode discrete values from step 1. We will use **OneHotEncoder** https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html\n",
        "\n",
        "```\n",
        "  #  id | category       | category_index | category_a | category_b\n",
        "  # ----|----------------|----------------|------------|-----------\n",
        "  #  0  | a              | 0.0            |1           |0\n",
        "  #  1  | b              | 2.0            |0           |1\n",
        "  #  2  | c              | 1.0            |0           |0\n",
        "  #  3  | a              | 0.0            |1           |0\n",
        "  #  4  | a              | 0.0            |1           |0\n",
        "  #  5  | c              | 1.0            |0           |0\n",
        "```\n",
        "When handleInvalid is configured to ‘keep’, an extra “category” indicating invalid values is added as last category.\n",
        "\n",
        "```\n",
        "  #  id | category       | category_index | category_a | category_b | category_c\n",
        "  # ----|----------------|----------------|------------|------------|-----------\n",
        "  #  0  | a              | 0.0            |1           |0           |0\n",
        "  #  1  | b              | 2.0            |0           |1           |0\n",
        "  #  2  | c              | 1.0            |0           |0           |1\n",
        "  #  3  | a              | 0.0            |1           |0           |0\n",
        "  #  4  | a              | 0.0            |1           |0           |0\n",
        "  #  5  | c              | 1.0            |0           |0           |1\n",
        "```\n",
        "\n",
        "3. Continuous array variables stay unchanged .\n",
        "4. All features need to be collected in one vector, that will be used later on for model training. We will use **VectorAssembler** https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html?highlight=vectorassembler \n",
        "\n",
        "```\n",
        "  #  id | category       | category_index | category_ohe_vector\n",
        "  # ----|----------------|----------------|---------------------\n",
        "  #  0  | a              | 0.0            |[1, 0, 0]\n",
        "  #  1  | b              | 2.0            |[0, 1, 0]\n",
        "  #  2  | c              | 1.0            |[0, 0, 1]\n",
        "  #  3  | a              | 0.0            |[1, 0, 0]\n",
        "  #  4  | a              | 0.0            |[1, 0, 0]\n",
        "  #  5  | c              | 1.0            |[0, 0, 1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im0jFfxfSsYE"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6sX9fPuuqmK"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "categorical_col_names = [\"workclass\", \"education\", \"marital-status\", \n",
        "             \"occupation\", \"relationship\", \"race\", \"sex\",\n",
        "             \"native-country\"]\n",
        "\n",
        "pipeline_steps = []\n",
        "\n",
        "for categorical_col_name in categorical_col_names:\n",
        "  indexer = StringIndexer(inputCol=categorical_col_name, outputCol=categorical_col_name+\"_index\", handleInvalid='keep') \n",
        "  encoder = OneHotEncoder(inputCol=categorical_col_name+\"_index\", outputCol=categorical_col_name+\"_class\")\n",
        "  pipeline_steps += [indexer, encoder]\n",
        "\n",
        "\n",
        "indexer_label = StringIndexer(inputCol=\"earnings\", outputCol=\"earnings_index\") \n",
        "pipeline_steps += [indexer_label]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S4JBT0AS07e"
      },
      "source": [
        "## Features vector creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNNi3CD4P9cc"
      },
      "source": [
        "We do not add earnings_index to vector of features since it is our label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxd41WLV13m8"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "numeric_col_names = [\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
        "assembler_inputs = list(map(lambda c: c + \"_class\", categorical_col_names))+ numeric_col_names\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "pipeline_steps += [assembler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXhwep7UVLML"
      },
      "source": [
        "# dataset.select('relationship').distinct().collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8sNG1WVS35a"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogmXt8HjYAmi"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "regressor = LogisticRegression(labelCol=\"earnings_index\", featuresCol=\"features\", maxIter=50)\n",
        "pipeline_steps += [regressor]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rDpU5jNS9LX"
      },
      "source": [
        "## Fit pipeline on train data and do prediction on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JnF9c3kZloT"
      },
      "source": [
        "Transform is like scikit predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQVLMbBDYS-V"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=pipeline_steps)\n",
        "\n",
        "model = pipeline.fit(trainingData)\n",
        "predictions = model.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZUt1K2-IR1J"
      },
      "source": [
        "predictions.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Coefficients:', model.stages[-1].coefficients)\n",
        "print('Model Intercept: ', model.stages[-1].intercept)"
      ],
      "metadata": {
        "id": "DiuXcjfnxllh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC3iVK0xY-eR"
      },
      "source": [
        "selected = predictions.select(\"earnings_index\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "selected.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mOWCjnfTKtn"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aubOYI9tQd46"
      },
      "source": [
        "## **BinaryClassificationEvaluator**\n",
        "\n",
        "https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z46cjTWBIWRa"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='earnings_index')\n",
        "\n",
        "for line in evaluator.explainParams().split('\\n'):\n",
        " print(line)\n",
        " \n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_OV030IbaR"
      },
      "source": [
        "evaluator.getMetricName()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbpYYJXNTO-9"
      },
      "source": [
        "## RegressionEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrb4qAtzVoGj"
      },
      "source": [
        "The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgPmzMwpRAbP"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol='earnings_index')\n",
        "\n",
        "for line in evaluator.explainParams().split('\\n'):\n",
        " print(line)\n",
        "\n",
        "print('RMSE:', evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
        "print('R-squared:', evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"}))\n",
        "print('mae:', evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_usmCZ6SIkI"
      },
      "source": [
        "# Hyper Parameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdqSY7uUIpK8"
      },
      "source": [
        "for line in regressor.explainParams().split('\\n'):\n",
        " print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I-eyC_yTUhd"
      },
      "source": [
        "Create ParamGrid for Cross Validation (In Sklearn gridsearchCV)\n",
        "\n",
        "We use 5-fold CrossValidator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh_Cun4IaRZ0"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(regressor.regParam, [0.01, 0.5, 2.0])\n",
        "             .addGrid(regressor.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .addGrid(regressor.maxIter, [1, 5, 10, 100])\n",
        "             .build())\n",
        "\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYyB8c4bTi3x"
      },
      "source": [
        "Run cross validations, may take time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRSXWDBeSew"
      },
      "source": [
        "cvModel = cv.fit(trainingData) # usually takes up to 12 minutes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NA1CKWRTpeU"
      },
      "source": [
        "Use test set here so we can measure the accuracy of our model on new data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWv400QpatFW"
      },
      "source": [
        "predictions = cvModel.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpia6hnsTuhl"
      },
      "source": [
        "cvModel always uses the best model found from the Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m76bZyZzatht"
      },
      "source": [
        "print('RMSE:', evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
        "print('R-squared:', evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"}))\n",
        "print('mae:', evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW0p1P97axvB"
      },
      "source": [
        "print('Model Intercept: ', cvModel.bestModel.stages[-1].intercept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQNEJ7lYT1Sp"
      },
      "source": [
        "Check best model's predictions and probabilities of each prediction class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgpPbnDua1Az"
      },
      "source": [
        "selected = predictions.select(\"earnings_index\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
        "selected.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7St4RcBWV3-4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}